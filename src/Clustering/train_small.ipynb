{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from razdel import sentenize\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelegramRegressionReader(Dataset):\n",
    "    def __init__(self, txt_path, vec_path, chunk_size=2048):\n",
    "        self.txt_path = txt_path\n",
    "        self.vec_path = vec_path\n",
    "\n",
    "        self.shift = 3200 # numpy load reads 3200 bytes from file handler which is equal one vector\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        self.size = sum(\n",
    "            len(el) for el in pd.read_json(\n",
    "                self.txt_path,\n",
    "                encoding='utf-8',\n",
    "                lines=True,\n",
    "                chunksize=chunk_size)\n",
    "        )\n",
    "        \n",
    "        s = [0]\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            self.txt_linelocs = [s.append(s[0]+len(n)) or s.pop(0) for n in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert type(idx) == int\n",
    "\n",
    "        with open(self.txt_path, 'r', encoding='utf-8') as f_txt,\\\n",
    "             open(self.vec_path, 'rb') as f_vec:\n",
    "            f_txt.seek(self.txt_linelocs[idx], 0)\n",
    "            txt = f_txt.readline()\n",
    "            \n",
    "            f_vec.seek(self.shift * idx, 0)\n",
    "            vec = np.load(f_vec).reshape(1, -1)\n",
    "            \n",
    "            sample = {'text': txt, 'vector': vec}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/texts.jsonl',\n",
    "                                '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 ms, sys: 522 µs, total: 3.67 ms\n",
      "Wall time: 2.36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample = data[111052]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0047325664,\n",
       " 0.3477171,\n",
       " 1.1353046,\n",
       " array([-0.06912806, -0.3473212 ,  0.01208007,  0.21316218, -0.11906805,\n",
       "         0.11900068, -0.16061948, -0.18539721], dtype=float32))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['vector'].mean(), sample['vector'].std(), sample['vector'].max(), sample['vector'][0][::100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'тегеран. на следующей неделе национальная иранская нефтяная компания (nioc) планирует выставить на продажу 6 млн баррелей нефти и газового конденсата. аукцион будет проходить на иранской энергетической бирже (irenex). в понедельник nioc предложит 2 млн баррелей газового конденсата по базовой цене $57,87 за баррель. на следующий день, 19 ноября, компания продаст 2 млн баррелей легкой нефти по базовой цене $ 56,72 за баррель, а в среду, 20 ноября, выставит на продажу 2 млн баррелей тяжелой нефти, уточняет «иран.ру». закон о бюджете ирана на текущий 1398 календарный год, который начался 21 марта, обязывает министерство нефти ирана продавать на irenex не менее 2 млн баррелей легкой сырой нефти, 2 миллиона баррелей тяжелой сырой нефти и 2 млн баррелей газового конденсата. nioc планирует выставить на продажу 6 млн баррелей нефти и газового конденсата'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(sample['text'])['text'] + ' ' + json.loads(sample['text'])['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with pretrained FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        vec = list(map(float, tokens[1:]))\n",
    "        vec = np.array(vec)\n",
    "        data[tokens[0]] = vec\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vecs = load_vectors(\"/data/alolbuhtijarov/wiki-news-300d-1M-subword.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_token_vectors = torch.FloatTensor([\n",
    "    ft_vecs.get(w) if w in ft_vecs else np.random.rand(300) * 1e-3 for w in tokens\n",
    "])\n",
    "\n",
    "vocab_token_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=128,\n",
    "                freeze_embed=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(vocab_token_vectors, freeze=freeze_embed)\n",
    "\n",
    "        self.layer_title = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=300, out_channels=300, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            nn.BatchNorm1d(num_features=300),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer_text = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=300, out_channels=300, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            nn.BatchNorm1d(num_features=300),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer_cat = nn.Sequential(\n",
    "            nn.Linear(n_cat_features, hid_size * 2),\n",
    "            nn.BatchNorm1d(num_features=hid_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size * 2, hid_size),\n",
    "            nn.BatchNorm1d(num_features=hid_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Linear(300 * 2 + hid_size, 1)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title = self.embed(batch[\"Title\"])\n",
    "        title = title.permute(0, 2, 1)\n",
    "        title = self.layer_title(title).squeeze(-1)\n",
    "\n",
    "        text = self.embed(batch[\"FullDescription\"])\n",
    "        text = text.permute(0, 2, 1)\n",
    "        text = self.layer_text(text).squeeze(-1)\n",
    "\n",
    "        cat = self.layer_cat(batch[\"Categorical\"])\n",
    "\n",
    "        x = torch.cat([title, text, cat], dim=1)\n",
    "        return self.predictor(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/optim.html#per-parameter-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_optimizer(net):\n",
    "    embed_param = [kv[1] for kv in net.named_parameters() if kv[0] == 'embed.weight']\n",
    "    model_params = [kv[1] for kv in net.named_parameters() if kv[0] != 'embed.weight']\n",
    "    opt = torch.optim.Adam([\n",
    "                {'params': model_params},\n",
    "                {'params': embed_param, 'lr': 3e-4}\n",
    "    ], lr=3e-3)\n",
    "    return opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sum",
   "language": "python",
   "name": "env_sum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
