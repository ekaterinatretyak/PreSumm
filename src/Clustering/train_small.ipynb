{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from razdel import sentenize\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "MAX_LEN = 200\n",
    "UNK, PAD = \"UNK\", \"PAD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelegramRegressionReader(Dataset):\n",
    "    def __init__(self, txt_path, vec_path, chunk_size=2048):\n",
    "        self.txt_path = txt_path\n",
    "        self.vec_path = vec_path\n",
    "\n",
    "        self.shift = 3200 # numpy load reads 3200 bytes from file handler which is equal one vector\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        self.size = sum(\n",
    "            len(el) for el in pd.read_json(\n",
    "                self.txt_path,\n",
    "                encoding='utf-8',\n",
    "                lines=True,\n",
    "                chunksize=chunk_size)\n",
    "        )\n",
    "        \n",
    "        s = [0]\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            self.txt_linelocs = [s.append(s[0]+len(n)+1) or s.pop(0) for n in f]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.txt_path, 'r', encoding='utf-8') as f_txt,\\\n",
    "             open(self.vec_path, 'rb') as f_vec:\n",
    "            f_txt.seek(self.txt_linelocs[idx], 0)\n",
    "            txt = f_txt.readline()\n",
    "\n",
    "            f_vec.seek(self.shift * idx, 0)\n",
    "            vec = np.load(f_vec).reshape(1, -1)\n",
    "            \n",
    "            return {'text': txt, 'vector': vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/train_texts.jsonl',\n",
    "                                '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/train_vec.npy')\n",
    "\n",
    "test = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/test_texts.jsonl',\n",
    "                                '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/test_vec.npy')\n",
    "\n",
    "val = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/val_texts.jsonl',\n",
    "                              '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/val_vec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456939, 14231, 9813)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)\n",
    "val_loader = DataLoader(val, batch_size=512, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=512, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_token_list(txt_dict):\n",
    "    txt = txt_dict['text'] + ' ' + txt_dict['title']\n",
    "    return wordpunct_tokenize(txt.replace('\\xa0', ' ').lower().strip())\n",
    "\n",
    "def sample_to_token_list(sample):\n",
    "    return dict_to_token_list(json.loads(sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 456939/456939 [04:45<00:00, 1599.31it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "for i in tqdm.trange(len(train)):\n",
    "    cnt.update(sample_to_token_list(train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948538"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [UNK, PAD] + [el[0] for el in cnt.most_common(50000)]\n",
    "token_to_id = {t: i for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with pretrained FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('/data/alolbuhtijarov/fasttext_pretrained/cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50002, 300])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_token_vectors = torch.FloatTensor([\n",
    "    ft.get_word_vector('w') for w in tokens\n",
    "])\n",
    "\n",
    "vocab_token_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_token_vectors[UNK_IX] = 0\n",
    "vocab_token_vectors[PAD_IX] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_txt_to_input_inds(json_dict_txt):\n",
    "    tokens = dict_to_token_list(json.loads(json_dict_txt))\n",
    "    if len(tokens) < MAX_LEN:\n",
    "        tokens = tokens + [PAD] * (MAX_LEN - len(tokens))\n",
    "\n",
    "    return torch.LongTensor([token_to_id.get(word, UNK_IX) for word in tokens[:MAX_LEN]])\n",
    "\n",
    "def batch_to_torch_x_y(batch):\n",
    "    x = list(map(json_txt_to_input_inds, batch['text']))\n",
    "    x = torch.cat(x).view(-1, MAX_LEN).to(DEVICE)\n",
    "    y = torch.FloatTensor(batch['vector']).squeeze(1).to(DEVICE)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def raw_txt_to_input_inds(txt):\n",
    "    tokens = wordpunct_tokenize(txt.replace('\\xa0', ' ').lower().strip())\n",
    "    if len(tokens) < MAX_LEN:\n",
    "        tokens = tokens + [PAD] * (MAX_LEN - len(tokens))\n",
    "\n",
    "    return torch.LongTensor([token_to_id.get(word, UNK_IX) for word in tokens[:MAX_LEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, data_loader, batch_size=256):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    cos_loss_val = 0\n",
    "    cos_loss = nn.CosineEmbeddingLoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, y = batch_to_torch_x_y(batch)\n",
    "            batch_pred = model(x)\n",
    "            squared_error += torch.sum(torch.square(batch_pred - y))            \n",
    "            cos_loss_val += cos_loss(batch_pred, y, torch.ones(len(y)).to(DEVICE)).item()\n",
    "            abs_error += torch.sum(torch.abs(batch_pred - y))\n",
    "            num_samples += len(y)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "    print(\"Mean square error: %.5f\" % mse)\n",
    "    print(\"Mean absolute error: %.5f\" % mae)\n",
    "    print(\"Cosine loss: %.5f\" % cos_loss_val)\n",
    "    return mse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens),\n",
    "                 hid_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(vocab_token_vectors, freeze=False)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=300, out_channels=300, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            nn.BatchNorm1d(num_features=300),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.ff = nn.Linear(300, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.layers(x).squeeze(-1)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallEncoder().to(DEVICE)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [02:46,  2.69it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 294.65074\n",
      "Mean absolute error: 374.39600\n",
      "Cosine loss: 23.11614\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [02:45,  2.71it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 208.58994\n",
      "Mean absolute error: 314.59602\n",
      "Cosine loss: 19.32831\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [02:51,  2.61it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 191.15101\n",
      "Mean absolute error: 299.37562\n",
      "Cosine loss: 18.57789\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [02:48,  2.65it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 139.27867\n",
      "Mean absolute error: 255.43291\n",
      "Cosine loss: 16.55982\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [02:41,  2.77it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 97.44005\n",
      "Mean absolute error: 215.37186\n",
      "Cosine loss: 14.48609\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "    run_loss = None\n",
    "    model.train()\n",
    "    for i, batch in tqdm.tqdm(enumerate(train_loader), total=len(train) // BATCH_SIZE):\n",
    "        x, y = batch_to_torch_x_y(batch)\n",
    "        pred = model(x)\n",
    "\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if run_loss is None:\n",
    "            run_loss = loss.item()\n",
    "            \n",
    "        run_loss = 0.9 * run_loss + 0.1 * loss.item()\n",
    "    \n",
    "    print_metrics(model, val_loader)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 97.35272\n",
      "Mean absolute error: 215.27334\n",
      "Cosine loss: 10.32175\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model, test_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_clustering import eval_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallEncoder(\n",
       "  (embed): Embedding(50002, 300)\n",
       "  (layers): Sequential(\n",
       "    (0): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
       "    (1): AdaptiveAvgPool1d(output_size=1)\n",
       "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (ff): Linear(in_features=300, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_vec(txt):\n",
    "    with torch.no_grad():\n",
    "        vec = model(raw_txt_to_input_inds(txt).to(DEVICE).unsqueeze(0))\n",
    "    \n",
    "    return vec.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation.evaluate_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation.evaluate_clustering' from '/home/alolbuhtijarov/PreSumm/src/Clustering/evaluation/evaluate_clustering.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(evaluation.evaluate_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "136it [00:00, 1356.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "292it [00:00, 1409.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "427it [00:00, 1388.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "562it [00:00, 1374.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "708it [00:00, 1396.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "858it [00:00, 1424.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1006it [00:00, 1438.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1158it [00:00, 1459.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1300it [00:00, 1446.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1446it [00:01, 1449.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1596it [00:01, 1463.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1745it [00:01, 1470.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "1893it [00:01, 1470.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2053it [00:01, 1505.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2209it [00:01, 1519.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2368it [00:01, 1537.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2527it [00:01, 1551.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2683it [00:01, 1528.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2836it [00:01, 1456.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "2983it [00:02, 1419.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3129it [00:02, 1431.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3280it [00:02, 1453.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3429it [00:02, 1463.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3585it [00:02, 1490.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3739it [00:02, 1504.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "3890it [00:02, 1494.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4045it [00:02, 1508.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4198it [00:02, 1514.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4354it [00:02, 1525.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4507it [00:03, 1526.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4660it [00:03, 1516.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4812it [00:03, 1503.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "4963it [00:03, 1485.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5115it [00:03, 1494.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5265it [00:03, 1450.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5411it [00:03, 1436.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5558it [00:03, 1446.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5708it [00:03, 1461.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "5855it [00:03, 1463.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6010it [00:04, 1481.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6164it [00:04, 1496.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6315it [00:04, 1499.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6473it [00:04, 1520.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6626it [00:04, 1520.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6779it [00:04, 1522.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "6932it [00:04, 1521.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7087it [00:04, 1528.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7240it [00:04, 1510.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7392it [00:04, 1495.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7542it [00:05, 1495.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7692it [00:05, 1438.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7837it [00:05, 1392.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "7977it [00:05, 1394.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8118it [00:05, 1396.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8260it [00:05, 1402.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8403it [00:05, 1409.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8545it [00:05, 1399.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8689it [00:05, 1410.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8841it [00:06, 1441.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "8990it [00:06, 1453.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9136it [00:06, 1455.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9282it [00:06, 1449.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9428it [00:06, 1451.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9576it [00:06, 1459.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9723it [00:06, 1461.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "9871it [00:06, 1466.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "10018it [00:06, 1411.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "10160it [00:06, 1402.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "10308it [00:07, 1424.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "10458it [00:07, 1444.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "10730it [00:07, 1466.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 1/11 [00:23<03:57, 23.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 2/11 [00:47<03:33, 23.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 3/11 [01:11<03:10, 23.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 4/11 [01:35<02:47, 23.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 5/11 [02:00<02:24, 24.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▍    | 6/11 [02:24<02:00, 24.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▎   | 7/11 [02:48<01:36, 24.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 8/11 [03:12<01:12, 24.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 9/11 [03:36<00:47, 23.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 10/11 [03:59<00:23, 23.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 11/11 [04:23<00:00, 23.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "12it [04:48, 24.01s/it]                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "13it [05:11, 23.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "14it [05:35, 23.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "15it [05:59, 23.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "16it [06:23, 23.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A/home/alolbuhtijarov/PreSumm/env_sum/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17it [06:48, 24.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A/home/alolbuhtijarov/PreSumm/env_sum/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18it [07:15, 25.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A/home/alolbuhtijarov/PreSumm/env_sum/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19it [07:42, 25.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A/home/alolbuhtijarov/PreSumm/env_sum/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20it [08:06, 24.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 1/11 [00:25<04:15, 25.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 2/11 [00:52<03:52, 25.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 3/11 [01:20<03:32, 26.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▋      | 4/11 [01:47<03:06, 26.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 5/11 [02:15<02:42, 27.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▍    | 6/11 [02:41<02:14, 26.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▎   | 7/11 [03:09<01:48, 27.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 8/11 [03:37<01:22, 27.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 9/11 [04:05<00:54, 27.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 10/11 [04:30<00:26, 26.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 11/11 [04:58<00:00, 27.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distance = 0.07705595090766043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82      1571\n",
      "           1       0.78      0.66      0.72      1130\n",
      "\n",
      "    accuracy                           0.78      2701\n",
      "   macro avg       0.78      0.76      0.77      2701\n",
      "weighted avg       0.78      0.78      0.78      2701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_clustering(txt_to_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/optim.html#per-parameter-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_optimizer(net):\n",
    "    embed_param = [kv[1] for kv in net.named_parameters() if kv[0] == 'embed.weight']\n",
    "    model_params = [kv[1] for kv in net.named_parameters() if kv[0] != 'embed.weight']\n",
    "    opt = torch.optim.Adam([\n",
    "                {'params': model_params},\n",
    "                {'params': embed_param, 'lr': 3e-4}\n",
    "    ], lr=3e-3)\n",
    "    return opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sum",
   "language": "python",
   "name": "env_sum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
