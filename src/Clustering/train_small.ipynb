{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from razdel import sentenize\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelegramRegressionReader(Dataset):\n",
    "    def __init__(self, txt_path, vec_path, chunk_size=2048):\n",
    "        self.txt_path = txt_path\n",
    "        self.vec_path = vec_path\n",
    "\n",
    "        self.shift = 3200 # numpy load reads 3200 bytes from file handler which is equal one vector\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        self.size = sum(\n",
    "            len(el) for el in pd.read_json(\n",
    "                self.txt_path,\n",
    "                encoding='utf-8',\n",
    "                lines=True,\n",
    "                chunksize=chunk_size)\n",
    "        )\n",
    "        \n",
    "        s = [0]\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            self.txt_linelocs = [s.append(s[0]+len(n)+1) or s.pop(0) for n in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx - list of indexes\n",
    "        assert type(idx) == np.ndarray\n",
    "        samples = []\n",
    "\n",
    "        with open(self.txt_path, 'r', encoding='utf-8') as f_txt,\\\n",
    "             open(self.vec_path, 'rb') as f_vec:\n",
    "            for pos in idx:\n",
    "                f_txt.seek(self.txt_linelocs[pos], 0)\n",
    "                txt = f_txt.readline()\n",
    "\n",
    "                f_vec.seek(self.shift * pos, 0)\n",
    "                vec = np.load(f_vec).reshape(1, -1)\n",
    "\n",
    "                samples.append({'text': txt, 'vector': vec})\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/train_texts.jsonl',\n",
    "                                 '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/train_vec.npy')\n",
    "\n",
    "test = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/test_texts.jsonl',\n",
    "                                '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/test_vec.npy')\n",
    "\n",
    "val = TelegramRegressionReader('/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/val_texts.jsonl',\n",
    "                               '/data/alolbuhtijarov/datasets/BertSumAbs_predictions/split/val_vec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456939, 14231, 9813)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=256,, sampler=None, \n",
    "                          batch_sampler=None, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_token_list(sample):\n",
    "    txt_dict = json.loads(sample['text'])\n",
    "    txt = txt_dict['text'] + ' ' + txt_dict['title']\n",
    "    return wordpunct_tokenize(txt.replace('\\xa0', ' ').lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 456939/456939 [05:52<00:00, 1295.58it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "for i in tqdm.trange(len(train)):\n",
    "    ind = np.array([i])\n",
    "    cnt.update(sample_to_token_list(train[ind][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948538"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + [el[0] for el in cnt.most_common(50000)]\n",
    "token_to_id = {t: i for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder with pretrained FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('/data/alolbuhtijarov/fasttext_pretrained/cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50002, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_token_vectors = torch.FloatTensor([\n",
    "    ft.get_word_vector('w') for w in tokens\n",
    "])\n",
    "\n",
    "vocab_token_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=200):\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_token_vectors[0] = 0\n",
    "vocab_token_vectors[1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        tokens = sample_to_token_list(data[i])\n",
    "        vec = data[i]['vector']\n",
    "        \n",
    "        x.append(tokens)\n",
    "        y.append(vec)\n",
    "        \n",
    "\n",
    "    x = np.array(x)\n",
    "    x = as_matrix(x)\n",
    "    x = apply_word_dropout(x)\n",
    "    return torch.LongTensor(x).to(DEVICE), torch.FloatTensor(y).to(DEVICE).squeeze(1)\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop=0.9, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data[indices[start: start + batch_size]])\n",
    "            yield batch\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=256):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    cos_loss_val = 0\n",
    "    cos_loss = nn.CosineEmbeddingLoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in iterate_minibatches(data, batch_size=batch_size, shuffle=False):\n",
    "            batch_pred = model(x)\n",
    "            squared_error += torch.sum(torch.square(batch_pred - y))            \n",
    "            cos_loss_val += cos_loss(batch_pred, y, torch.ones(len(y)).to(DEVICE)).item()\n",
    "            abs_error += torch.sum(torch.abs(batch_pred - y))\n",
    "            num_samples += len(y)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "    print(\"Mean square error: %.5f\" % mse)\n",
    "    print(\"Mean absolute error: %.5f\" % mae)\n",
    "    print(\"Cosine loss: %.5f\" % cos_loss_val)\n",
    "    return mse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens),\n",
    "                 hid_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(vocab_token_vectors, freeze=False)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=300, out_channels=300, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            nn.BatchNorm1d(num_features=300),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.ff = nn.Linear(300, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.layers(x).squeeze(-1)\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallEncoder().to(DEVICE)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/223 [00:58<3:38:12, 58.97s/it]\u001b[A\n",
      "  1%|          | 2/223 [02:00<3:40:33, 59.88s/it]\u001b[A\n",
      "  1%|▏         | 3/223 [02:53<3:31:53, 57.79s/it]\u001b[A\n",
      "  2%|▏         | 4/223 [03:52<3:31:19, 57.90s/it]\u001b[A\n",
      "  2%|▏         | 5/223 [04:40<3:19:55, 55.03s/it]\u001b[A\n",
      "  3%|▎         | 6/223 [05:28<3:11:52, 53.05s/it]\u001b[A\n",
      "  3%|▎         | 7/223 [06:45<3:37:01, 60.28s/it]\u001b[A\n",
      "  4%|▎         | 8/223 [07:42<3:32:00, 59.16s/it]\u001b[A\n",
      "  4%|▍         | 9/223 [08:30<3:18:34, 55.67s/it]\u001b[A\n",
      "  4%|▍         | 10/223 [09:17<3:09:08, 53.28s/it]\u001b[A\n",
      "  5%|▍         | 11/223 [10:04<3:01:06, 51.26s/it]\u001b[A\n",
      "  5%|▌         | 12/223 [11:12<3:18:26, 56.43s/it]\u001b[A\n",
      "  6%|▌         | 13/223 [11:57<3:05:14, 52.93s/it]\u001b[A\n",
      "  6%|▋         | 14/223 [12:40<2:53:52, 49.92s/it]\u001b[A\n",
      "  7%|▋         | 15/223 [13:34<2:57:43, 51.27s/it]\u001b[A\n",
      "  7%|▋         | 16/223 [14:24<2:55:22, 50.84s/it]\u001b[A\n",
      "  8%|▊         | 17/223 [15:26<3:06:12, 54.23s/it]\u001b[A\n",
      "  8%|▊         | 18/223 [16:24<3:09:03, 55.34s/it]\u001b[A\n",
      "  9%|▊         | 19/223 [17:15<3:03:02, 53.84s/it]\u001b[A\n",
      "  9%|▉         | 20/223 [18:35<3:29:36, 61.95s/it]\u001b[A\n",
      "  9%|▉         | 21/223 [19:32<3:23:21, 60.40s/it]\u001b[A\n",
      " 10%|▉         | 22/223 [20:14<3:03:21, 54.73s/it]\u001b[A\n",
      " 10%|█         | 23/223 [20:52<2:45:53, 49.77s/it]\u001b[A\n",
      " 11%|█         | 24/223 [21:31<2:34:20, 46.54s/it]\u001b[A\n",
      " 11%|█         | 25/223 [22:11<2:26:54, 44.52s/it]\u001b[A\n",
      " 12%|█▏        | 26/223 [22:50<2:21:25, 43.08s/it]\u001b[A\n",
      " 12%|█▏        | 27/223 [23:28<2:15:03, 41.34s/it]\u001b[A\n",
      " 13%|█▎        | 28/223 [24:06<2:11:09, 40.36s/it]\u001b[A\n",
      " 13%|█▎        | 29/223 [24:43<2:07:01, 39.29s/it]\u001b[A\n",
      " 13%|█▎        | 30/223 [25:20<2:04:19, 38.65s/it]\u001b[A\n",
      " 14%|█▍        | 31/223 [25:58<2:03:45, 38.67s/it]\u001b[A\n",
      " 14%|█▍        | 32/223 [26:35<2:01:09, 38.06s/it]\u001b[A\n",
      " 15%|█▍        | 33/223 [27:11<1:58:26, 37.40s/it]\u001b[A\n",
      " 15%|█▌        | 34/223 [27:45<1:55:05, 36.54s/it]\u001b[A\n",
      " 16%|█▌        | 35/223 [28:34<2:05:17, 39.99s/it]\u001b[A\n",
      " 16%|█▌        | 36/223 [29:09<2:00:22, 38.62s/it]\u001b[A\n",
      " 17%|█▋        | 37/223 [29:43<1:55:04, 37.12s/it]\u001b[A\n",
      " 17%|█▋        | 38/223 [30:17<1:51:49, 36.27s/it]\u001b[A\n",
      " 17%|█▋        | 39/223 [30:50<1:47:55, 35.19s/it]\u001b[A\n",
      " 18%|█▊        | 40/223 [31:23<1:45:32, 34.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09896427965586789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 41/223 [31:55<1:42:47, 33.89s/it]\u001b[A\n",
      " 19%|█▉        | 42/223 [32:27<1:40:06, 33.18s/it]\u001b[A\n",
      " 19%|█▉        | 43/223 [32:57<1:37:27, 32.49s/it]\u001b[A\n",
      " 20%|█▉        | 44/223 [33:37<1:43:10, 34.58s/it]\u001b[A\n",
      " 20%|██        | 45/223 [34:13<1:44:07, 35.10s/it]\u001b[A\n",
      " 21%|██        | 46/223 [34:46<1:41:39, 34.46s/it]\u001b[A\n",
      " 21%|██        | 47/223 [35:18<1:38:24, 33.55s/it]\u001b[A\n",
      " 22%|██▏       | 48/223 [35:47<1:34:33, 32.42s/it]\u001b[A\n",
      " 22%|██▏       | 49/223 [36:16<1:30:54, 31.35s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1d0a50e394d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     for i, (x, y) in tqdm.tqdm(enumerate(\n\u001b[1;32m      6\u001b[0m             iterate_minibatches(train, batch_size=BATCH_SIZE)),\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         ):\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PreSumm/env_sum/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7f4e42b5bc5d>\u001b[0m in \u001b[0;36miterate_minibatches\u001b[0;34m(data, batch_size, shuffle)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-14ba25bdd114>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mf_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt_linelocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    run_loss = None\n",
    "    model.train()\n",
    "    for i, (x, y) in tqdm.tqdm(enumerate(\n",
    "            iterate_minibatches(train, batch_size=BATCH_SIZE)),\n",
    "            total=len(train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(x)\n",
    "\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if run_loss is None:\n",
    "            run_loss = loss.item()\n",
    "            \n",
    "        run_loss = 0.9 * run_loss + 0.1 * loss.item()\n",
    "        \n",
    "        if i % 40 == 39:\n",
    "            print(run_loss)\n",
    "\n",
    "    if epoch % 2 == 1:\n",
    "        print_metrics(model, val)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error: 79.35949\n",
      "Mean absolute error: 195.63906\n",
      "Cosine loss: 29.05058\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model, test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/optim.html#per-parameter-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_optimizer(net):\n",
    "    embed_param = [kv[1] for kv in net.named_parameters() if kv[0] == 'embed.weight']\n",
    "    model_params = [kv[1] for kv in net.named_parameters() if kv[0] != 'embed.weight']\n",
    "    opt = torch.optim.Adam([\n",
    "                {'params': model_params},\n",
    "                {'params': embed_param, 'lr': 3e-4}\n",
    "    ], lr=3e-3)\n",
    "    return opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sum",
   "language": "python",
   "name": "env_sum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
